{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.feature_engineering_final import X_Transformer\n",
    "from ipynb.fs.full.feature_engineering_final import TargetNormalizedRegressor\n",
    "import pandas as pd\n",
    "from ipynb.fs.full.feature_engineering_final import DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import make_scorer\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy.ma as ma\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import boxcox_normmax\n",
    "from scipy.special import boxcox1p\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y_true, y_pred, sample_weight=None, multioutput='uniform_average'):\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred, sample_weight, multioutput))\n",
    "    \n",
    "#I just want to make rmse positive\n",
    "flipped_score = make_scorer(rmsle, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5),scoring=None):\n",
    "   \n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes,scoring = scoring)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\"train.csv\",\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_loader.getX_train()\n",
    "Y_train = data_loader.getY_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_transformer = X_Transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = x_transformer.fit_transform(X_train,Y_train)\n",
    "kf = KFold(n_splits=10,shuffle = True,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = pd.DataFrame(X_train)\n",
    "#overfit = []\n",
    "#for i in X_train.columns:\n",
    "#    counts = X_train[i].value_counts()\n",
    "#    zeros = counts.iloc[0]\n",
    "#    if zeros / len(X_train) * 100 > 99.94:\n",
    "#        overfit.append(i)\n",
    "\n",
    "#overfit = list(overfit)\n",
    "#X_train = X_train.drop(overfit, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = pd.DataFrame(X_train)\n",
    "#numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "#skew_features = X_train.apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "#skew_features\n",
    "#high_skew = skew_features[skew_features > 0]\n",
    "#high_skew\n",
    "#skew_index = high_skew.index\n",
    "#for i in skew_index:\n",
    "#    X_train[i] = boxcox1p(X_train[i], boxcox_normmax(X_train[i] + 1)) \n",
    "#np.sort(skew_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Optimal number of features : %d\" % selector.n_features_)\n",
    "\n",
    "#plt.figure()\n",
    "#plt.xlabel(\"Number of features selected\")\n",
    "#plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "#plt.plot(range(1, (len(selector.grid_scores_))*10 ,10), selector.grid_scores_)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_selection_mask = np.loadtxt(\"features_mask.csv\",delimiter = ',')\n",
    "#filtered_indexes = list(map(lambda el2: el2[0],filter(lambda el: el[1]==0, enumerate(feature_selection_mask))))\n",
    "#X_train = np.delete(X_train,filtered_indexes,axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cross validation and predicted vs measured cross validation\n",
    "xgb_regressor_cross_val = TargetNormalizedRegressor(XGBRegressor(objective='reg:squarederror',eval_metric = 'rmse',learning_rate=0.01,n_estimators=3460,\n",
    "                                     max_depth=3, min_child_weight=0,\n",
    "                                     gamma=0, subsample=0.7,\n",
    "                                     colsample_bytree=0.7,\n",
    "                                     nthread=-1,\n",
    "                                     scale_pos_weight=1, seed=27,\n",
    "                                     reg_alpha=0.00006))\n",
    "scores = cross_val_score(xgb_regressor_cross_val, X_train, Y_train, cv=kf,n_jobs=-1,scoring = flipped_score)\n",
    "print(\"Rmse log average: %0.5f (+/- %0.5f)\" % (scores.mean(), scores.std() ))\n",
    "print(\"Rmse log median: %0.5f\" % (np.median(scores)))\n",
    "predicted = cross_val_predict(xgb_regressor_cross_val, X_train, Y_train, cv=kf,n_jobs=-1)\n",
    "print(scores)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(Y_train, predicted, edgecolors=(0, 0, 0))\n",
    "ax.plot([Y_train.min(), Y_train.max()], [Y_train.min(), Y_train.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning curve of best model\n",
    "plot_learning_curve(xgb_regressor_cross_val, \"Title\", X_train, Y_train, None, cv=kf, n_jobs=-1,scoring = flipped_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = {\n",
    "        'max_depth': [5],\n",
    "        'eta': [0.005],\n",
    "        'n_estimators' : [200],\n",
    "        #'gamma' : [0,0.1,0.5,1,5,10],\n",
    "        #'min_child_weight': [1,2,5,10,30,50],\n",
    "        #'max_delta_step' : [,10,100,1000,10000,100000,1000000000],\n",
    "        'subsample' : [0.5],\n",
    "        #'colsample_bytree' : [0.5,0.8,1],\n",
    "        #'colsample_bylevel' : [0.5,0.8,1],\n",
    "        #'colsample_bynode' : [0.5,0.8,1],\n",
    "        #'lambda' : [0.1,100],\n",
    "        #'alpha' : [0.1,100]\n",
    "        \n",
    "        }\n",
    "clf = GridSearchCV(XGBRegressor(objective='reg:squarederror',eval_metric = 'rmsle'), tuned_parameters, cv=kf,scoring='neg_mean_squared_log_error',n_jobs=-1)\n",
    "clf.fit(X_train, Y_train)\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(clf.best_params_)\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.5f (+/-%0.05f) for %r\" % (mean, std * 2, params))tuned_parameters = {\n",
    "        'max_depth': [5],\n",
    "        'eta': [0.005],\n",
    "        'n_estimators' : [200],\n",
    "        #'gamma' : [0,0.1,0.5,1,5,10],\n",
    "        #'min_child_weight': [1,2,5,10,30,50],\n",
    "        #'max_delta_step' : [,10,100,1000,10000,100000,1000000000],\n",
    "        'subsample' : [0.5],\n",
    "        #'colsample_bytree' : [0.5,0.8,1],\n",
    "        #'colsample_bylevel' : [0.5,0.8,1],\n",
    "        #'colsample_bynode' : [0.5,0.8,1],\n",
    "        #'lambda' : [0.1,100],\n",
    "        #'alpha' : [0.1,100]\n",
    "        \n",
    "        }\n",
    "clf = GridSearchCV(XGBRegressor(objective='reg:squarederror',eval_metric = 'rmsle'), tuned_parameters, cv=kf,scoring='neg_mean_squared_log_error',n_jobs=-1)\n",
    "clf.fit(X_train, Y_train)\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(clf.best_params_)\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.5f (+/-%0.05f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regressor = XGBRegressor(objective='reg:squarederror',eval_metric = 'rmsle',max_depth = 5,eta = 0.005,n_estimators = 200,subsample = 0.5)\n",
    "#regressor.fit(X_train, Y_train)\n",
    "#test_data = pd.read_csv('test.csv')\n",
    "#X_test = test_data.drop(['Id'], axis = 1)\n",
    "#X_test = x_transformer.transform(X_test)\n",
    "#X_test = np.delete(X_test,filtered_indexes,axis = 1)\n",
    "#Ypredicted = regressor.predict(X_test)\n",
    "#resultDf = pd.DataFrame()\n",
    "#resultDf['Id'] = test_data['Id']\n",
    "#resultDf['SalePrice'] = Ypredicted\n",
    "#resultDf\n",
    "#resultDf.to_csv('submission3.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.1, random_state=30)\n",
    "# fit model no training data\n",
    "#xgb_regressor_early_stopping = XGBRegressor(objective='reg:squarederror',eval_metric = 'rmse',max_depth = 3,eta = 0.005,n_estimators = 3500,subsample = 0.5)\n",
    "#eval_set = [(x_test, y_test)]\n",
    "#xgb_regressor_early_stopping.fit(x_train, y_train, eval_metric=\"rmse\", eval_set=eval_set,early_stopping_rounds = 3000)\n",
    "# make predictions for test data\n",
    "#y_pred = xgb_regressor_early_stopping.predict(x_test)\n",
    "# evaluate predictions\n",
    "#err = rmsle(y_test, y_pred)\n",
    "#print(\"rmsle: %.5f%%\" % (err))\n",
    "#print(\"n_tree: %s\" % (xgb_regressor_early_stopping.best_ntree_limit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

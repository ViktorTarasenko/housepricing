{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.feature_engineering_final import X_Transformer_scaled\n",
    "from ipynb.fs.full.feature_engineering_final import TargetNormalizedRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from ipynb.fs.full.feature_engineering_final import DataLoader\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from ipynb.fs.full.feature_engineering_final import BlendingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy.ma as ma\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import boxcox_normmax\n",
    "from scipy.special import boxcox1p\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y_true, y_pred, sample_weight=None, multioutput='uniform_average'):\n",
    "    return np.sqrt(mean_squared_log_error(y_true, y_pred, sample_weight, multioutput))\n",
    "    \n",
    "#I just want to make rmse positive\n",
    "flipped_score = make_scorer(rmsle, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5),scoring=None):\n",
    "   \n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes,scoring = scoring)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\"train.csv\",\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_loader.getX_train()\n",
    "Y_train = data_loader.getY_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_transformer = X_Transformer_scaled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = x_transformer.fit_transform(X_train)\n",
    "kf = KFold(n_splits=5,shuffle = True,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cross validation and predicted vs measured cross validation\n",
    "en = ElasticNetCV(max_iter=1e7, alphas=[0.0003,0.0004,0.00045,0.0005,0.0006], cv=10, l1_ratio=[0.6,0.7,0.8,0.9,0.91,0.92,0.93,0.94,0.95,0.96,0.97,0.98,0.99])\n",
    "meta = LassoCV(cv=10)\n",
    "#meta = ElasticNetCV(cv=10)\n",
    "kr =  KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=9.035)\n",
    "huber = HuberRegressor(alpha = 265,epsilon=1.6)\n",
    "ridge = RidgeCV(alphas=[14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5,15.5,15.6])\n",
    "bayesian_ridge = BayesianRidge()\n",
    "scvr = StackingCVRegressor(regressors=(en,kr,huber),\n",
    "                                meta_regressor=meta,cv=15,shuffle = True,random_state = 42,\n",
    "                                use_features_in_secondary=True,\n",
    "                                  n_jobs = -1)\n",
    "\n",
    "scvr_t = TargetNormalizedRegressor(scvr)\n",
    "\n",
    "\n",
    "\n",
    "br1 = BlendingRegressor(regressors_and_weights = [(en,0.65),(kr,0.35)])\n",
    "br = BlendingRegressor(regressors_and_weights = [(br1,0.78),(huber,0.22)])\n",
    "#br = BlendingRegressor(regressors_and_weights = [(br2,0.999),(ridge,0.001)])\n",
    "#br_t = TargetNormalizedRegressor(br)\n",
    "scores = cross_val_score(scvr_t, X_train, Y_train, cv=kf,n_jobs=-1,scoring = flipped_score)\n",
    "print(\"Rmse log average: %0.5f (+/- %0.5f)\" % (scores.mean(), scores.std() * 2))\n",
    "print(\"Rmse log median: %0.5f\" % (np.median(scores)))\n",
    "predicted = cross_val_predict(scvr_t, X_train, Y_train, cv=kf,n_jobs=-1)\n",
    "print(scores)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(Y_train, predicted, edgecolors=(0, 0, 0))\n",
    "ax.plot([Y_train.min(), Y_train.max()], [Y_train.min(), Y_train.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Measured')\n",
    "ax.set_ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning curve of best model\n",
    "plot_learning_curve(scvr_t, \"Title\", X_train, Y_train, None, cv=kf, n_jobs=-1,scoring = flipped_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

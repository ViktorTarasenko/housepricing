{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[data_processing]  train.csv\n",
      "[data_processing]  test.csv\n",
      "[data_processing]  The train data size before dropping Id: (1460, 81) \n",
      "[data_processing]  The test data size before dropping Id: (1459, 80) \n",
      "[data_processing]  The train data size after dropping Id: (1460, 80) \n",
      "[data_processing]  The test data size after dropping Id: (1459, 79) \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[data_processing]  all_data size is : (2917, 79)\n",
      "[data_processing]                Missing Ratio\n",
      "PoolQC            99.691464\n",
      "MiscFeature       96.400411\n",
      "Alley             93.212204\n",
      "Fence             80.425094\n",
      "FireplaceQu       48.680151\n",
      "LotFrontage       16.660953\n",
      "GarageFinish       5.450806\n",
      "GarageYrBlt        5.450806\n",
      "GarageQual         5.450806\n",
      "GarageCond         5.450806\n",
      "GarageType         5.382242\n",
      "BsmtExposure       2.811107\n",
      "BsmtCond           2.811107\n",
      "BsmtQual           2.776826\n",
      "BsmtFinType2       2.742544\n",
      "BsmtFinType1       2.708262\n",
      "MasVnrType         0.822763\n",
      "MasVnrArea         0.788481\n",
      "MSZoning           0.137127\n",
      "BsmtFullBath       0.068564\n",
      "BsmtHalfBath       0.068564\n",
      "Utilities          0.068564\n",
      "Functional         0.068564\n",
      "Exterior2nd        0.034282\n",
      "Exterior1st        0.034282\n",
      "SaleType           0.034282\n",
      "BsmtFinSF1         0.034282\n",
      "BsmtFinSF2         0.034282\n",
      "BsmtUnfSF          0.034282\n",
      "Electrical         0.034282\n",
      "[data_processing]  Shape all_data: (2917, 79)\n",
      "[data_processing]  (2917, 220)\n",
      "[model]  Lasso score(cv): 0.1117 (0.0073)\n",
      "[model]  ElasticNet score(cv): 0.1116 (0.0074)\n",
      "[model]  Kernel Ridge score(cv): 0.1158 (0.0075)\n",
      "[model]  Gradient Boosting score(cv): 0.1162 (0.0086)\n",
      "[model]  Xgboost score(cv): 0.1159 (0.0066)\n",
      "[model]  LGBM score(cv): 0.1170 (0.0060)\n",
      "[model]  Averaged base models score(cv): 0.1088 (0.0075)\n",
      "[model]  Stacked averaged-models score(cv): 0.1084 (0.0070)\n",
      "[model]  stacked regressor: 0.07797453048932439\n",
      "[model]  xgboost: 0.078912986885466\n",
      "[model]  LightGBMs: 0.07260912009060388\n",
      "[model]  ensemble: 0.07530483875062427\n",
      "The end!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n",
    "\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "\n",
    "### STEP1: settings\n",
    "class Settings(Enum):\n",
    "    train_path    = 'train.csv'\n",
    "    test_path     = 'test.csv'\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value\n",
    "    \n",
    "    \n",
    "### STEP2: data processing\n",
    "def display_outlier(pd, feature):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(x = pd[feature], y = pd['SalePrice'])\n",
    "    plt.ylabel('SalePrice', fontsize=13)\n",
    "    plt.xlabel(feature, fontsize=13)\n",
    "    plt.show()      \n",
    "        \n",
    "def display_distrib(pd, feature):\n",
    "    plt.figure()\n",
    "    sns.distplot(pd[feature].dropna() , fit=norm);\n",
    "    (mu, sigma) = norm.fit(pd[feature].dropna())    \n",
    "    \n",
    "    plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('SalePrice distribution')\n",
    "    plt.show()\n",
    "                        \n",
    "def process_data(train_path, test_path):\n",
    "    print('[data_processing] ', train_path)\n",
    "    print('[data_processing] ', test_path)\n",
    "    \n",
    "    global train\n",
    "    global test\n",
    "    global y_train\n",
    "    global train_ID\n",
    "    global test_ID\n",
    "    \n",
    "    # load data\n",
    "    train = pd.read_csv(str(train_path))\n",
    "    test = pd.read_csv(str(test_path))\n",
    "        \n",
    "    #print('[data_processing] ', train.head(5))\n",
    "    #print('[data_processing] ', test.head(5))\n",
    "    \n",
    "    # drop ID feature\n",
    "    print('[data_processing] ', 'The train data size before dropping Id: {} '.format(train.shape))\n",
    "    print('[data_processing] ', 'The test data size before dropping Id: {} '.format(test.shape))\n",
    "    \n",
    "    train_ID = train['Id']\n",
    "    test_ID = test['Id']\n",
    "    \n",
    "    train.drop('Id', axis = 1, inplace = True)\n",
    "    test.drop('Id', axis = 1, inplace = True)\n",
    "    \n",
    "    print('[data_processing] ', 'The train data size after dropping Id: {} '.format(train.shape))\n",
    "    print('[data_processing] ', 'The test data size after dropping Id: {} '.format(test.shape))    \n",
    "    \n",
    "    # analyze and remove huge outliers: GrLivArea, ...\n",
    "    display_outlier(train, 'GrLivArea')\n",
    "    train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n",
    "    display_outlier(train, 'GrLivArea')\n",
    "      \n",
    "    # normalize distribution of output (SalePrice)\n",
    "    display_distrib(train, 'SalePrice')\n",
    "    train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
    "    y_train = train.SalePrice.values\n",
    "    display_distrib(train, 'SalePrice')\n",
    "    \n",
    "    # concatenate the train and test data\n",
    "    ntrain = train.shape[0]\n",
    "    ntest = test.shape[0]\n",
    "    train.drop(['SalePrice'], axis=1, inplace=True)\n",
    "    all_data = pd.concat((train, test)).reset_index(drop=True)\n",
    "    print('[data_processing] ', 'all_data size is : {}'.format(all_data.shape))    \n",
    "    \n",
    "    # fill missing data\n",
    "    all_data_na = (all_data.isnull().sum() / len(all_data)) * 100\n",
    "    all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\n",
    "    missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n",
    "    print('[data_processing] ', missing_data)\n",
    "\n",
    "    all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\") #data description says NA means \"No Pool\". That make sense, given the huge ratio of missing value (+99%) and majority of houses have no Pool at all in general.\n",
    "    all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\") #data description says NA means \"no misc feature\"\n",
    "    all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\") #data description says NA means \"no alley access\"\n",
    "    all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\") #NA means \"no fence\"\n",
    "    all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\") #NA means \"no fireplace\"\n",
    "    all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))  # fill by the median LotFrontage of all neighborhood because they have same lot frontage\n",
    "    for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n",
    "        all_data[col] = all_data[col].fillna('None')\n",
    "    for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n",
    "        all_data[col] = all_data[col].fillna(0)\n",
    "    for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n",
    "        all_data[col] = all_data[col].fillna(0)\n",
    "    for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n",
    "        all_data[col] = all_data[col].fillna('None') #NaN means that there is no basement\n",
    "    all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\") #NA means no masonry veneer\n",
    "    all_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0) #NA means no masonry veneer\n",
    "    all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n",
    "    all_data = all_data.drop(['Utilities'], axis=1) #For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . Since the house with 'NoSewa' is in the training set, this feature won't help in predictive modelling. We can then safely remove it\n",
    "    all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\") #data description says NA means typical\n",
    "    all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0]) #It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.\n",
    "    all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0]) #Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the missing value in KitchenQual\n",
    "    all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0]) #Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string\n",
    "    all_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0]) #Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string\n",
    "    all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0]) #Fill in again with most frequent which is \"WD\"\n",
    "    all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\") #Na most likely means No building class. We can replace missing values with None\n",
    "    \n",
    "    # add important features more\n",
    "    all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF'] #feature which is the total area of basement, first and second floor areas of each house\n",
    "    \n",
    "    # normalize skewed features\n",
    "    for feature in all_data:\n",
    "        if all_data[feature].dtype != \"object\":\n",
    "                #display_distrib(all_data, feature)\n",
    "                all_data[feature] = np.log1p(all_data[feature])\n",
    "                #display_distrib(all_data, feature)\n",
    "                \n",
    "    # transform numeric features into categorical features\n",
    "    all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n",
    "    all_data['OverallQual'] = all_data['OverallQual'].astype(str)\n",
    "    all_data['OverallCond'] = all_data['OverallCond'].astype(str)\n",
    "    all_data['YrSold'] = all_data['YrSold'].astype(str)\n",
    "    all_data['MoSold'] = all_data['MoSold'].astype(str)\n",
    "    \n",
    "    # encode categorical features by LabelEncoder or dummies\n",
    "    # do label encoding for categorical features\n",
    "    categorical_features = \\\n",
    "    ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n",
    "     'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n",
    "     'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n",
    "     'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallQual',\n",
    "     'OverallCond', 'YrSold', 'MoSold')\n",
    "    for c in categorical_features:\n",
    "        lbl = LabelEncoder() \n",
    "        lbl.fit(list(all_data[c].values))\n",
    "        all_data[c] = lbl.transform(list(all_data[c].values))\n",
    "    print('[data_processing] ', 'Shape all_data: {}'.format(all_data.shape))\n",
    "    # get dummy categorical features\n",
    "    all_data = pd.get_dummies(all_data)\n",
    "    print('[data_processing] ', all_data.shape)\n",
    "    \n",
    "    train = all_data[:ntrain]\n",
    "    test = all_data[ntrain:]\n",
    "    \n",
    "    \n",
    "### STEP3: model\n",
    "class Averaging_Models(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    # now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([model.predict(X) for model in self.models_])\n",
    "        return np.mean(predictions, axis=1)   \n",
    "    \n",
    "class Stacking_Averaged_Models(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # we again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=15)\n",
    "        \n",
    "        # train cloned base models then create out-of-fold predictions that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    # do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)\n",
    "    \n",
    "n_folds = 5\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)\n",
    "\n",
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "\n",
    "def model():\n",
    "    # LASSO Regression\n",
    "    lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "    \n",
    "    score = rmsle_cv(lasso)\n",
    "    print('[model] ', \"Lasso score(cv): {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "    \n",
    "    # Elastic Net Regression\n",
    "    ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "    \n",
    "    score = rmsle_cv(ENet)\n",
    "    print('[model] ', \"ElasticNet score(cv): {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "    \n",
    "    # Kernel Ridge Regression\n",
    "    KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "    \n",
    "    score = rmsle_cv(KRR)\n",
    "    print('[model] ', \"Kernel Ridge score(cv): {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "    \n",
    "    # Gradient Boosting Regression\n",
    "    GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "    \n",
    "    score = rmsle_cv(GBoost)\n",
    "    print('[model] ', \"Gradient Boosting score(cv): {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "    \n",
    "    # XGBoost \n",
    "    model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1)\n",
    "    \n",
    "    score = rmsle_cv(model_xgb)\n",
    "    print('[model] ', \"Xgboost score(cv): {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "    \n",
    "    # LightGBM \n",
    "    model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
    "\n",
    "    score = rmsle_cv(model_lgb)\n",
    "    print('[model] ', \"LGBM score(cv): {:.4f} ({:.4f})\" .format(score.mean(), score.std()))\n",
    "    \n",
    "    # averaged model\n",
    "    averaged_models = Averaging_Models(models = (ENet, GBoost, KRR, lasso))    \n",
    "\n",
    "    score = rmsle_cv(averaged_models)\n",
    "    print('[model] ', \"Averaged base models score(cv): {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "    \n",
    "    # stacked averaged model\n",
    "    stacked_averaged_models = Stacking_Averaged_Models(base_models = (ENet, GBoost, KRR), meta_model = lasso)\n",
    "\n",
    "    score = rmsle_cv(stacked_averaged_models)\n",
    "    print('[model] ', \"Stacked averaged-models score(cv): {:.4f} ({:.4f})\".format(score.mean(), score.std()))    \n",
    "        \n",
    "    # StackedRegressor\n",
    "    stacked_averaged_models.fit(train.values, y_train)\n",
    "    stacked_train_pred = stacked_averaged_models.predict(train.values)\n",
    "    stacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\n",
    "    print('[model] ', 'stacked regressor:', rmsle(y_train, stacked_train_pred))\n",
    "\n",
    "    # XGBoost\n",
    "    model_xgb.fit(train, y_train)\n",
    "    xgb_train_pred = model_xgb.predict(train)\n",
    "    xgb_pred = np.expm1(model_xgb.predict(test))\n",
    "    print('[model] ', 'xgboost:', rmsle(y_train, xgb_train_pred))\n",
    "\n",
    "    # LightGBM\n",
    "    model_lgb.fit(train, y_train)\n",
    "    lgb_train_pred = model_lgb.predict(train)\n",
    "    lgb_pred = np.expm1(model_lgb.predict(test.values))\n",
    "    print('[model] ', 'LightGBMs:', rmsle(y_train, lgb_train_pred))\n",
    "\n",
    "    print('[model] ', 'ensemble:', rmsle(y_train,stacked_train_pred*0.70 + xgb_train_pred*0.15 + lgb_train_pred*0.15 ))\n",
    "\n",
    "    ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15\n",
    "    \n",
    "    sub = pd.DataFrame()\n",
    "    sub['Id'] = test_ID\n",
    "    sub['SalePrice'] = ensemble\n",
    "    sub.to_csv('submission_experimental.csv',index=False)\n",
    "\n",
    "\n",
    "### MAIN\n",
    "process_data(Settings.train_path, Settings.test_path)\n",
    "model()\n",
    "\n",
    "print('The end!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
